---
title: "data 621 assignment 1"
author: "Jun Pan"
date: "February 12, 2019"
output:
  html_document: default
  pdf_document: default
---

Objective: predict the number of wins for the baseball teams. 

    Baseballl is a kind of sports combines a sequence of pitches, at-bats, and innings where play is contained between discrete pitches. Unlike the more continuous play such as valleyball or tennis, this makes baseball conducive to gathering extensive data on individual and team performance. In this project, we attempt to model wins per season for Major League Baseball (MLB) teams . Our dataset includes 15 potential predictor variables, adjusted to reflect a standardized 162 game season, using MLB records from 1871 to 2006. Among the 15 variables, postive impact variables including: TEAM_BATTING_H, TEAM_BATTING_2B, TEAM_BATTING_3B, TEAM_BATTING_HR, TEAM_BATTING_BB, TEAM_BATTING_HBP, TEAM_BASERUN_SB, TEAM_FIELDING_DP, TEAM_PITCHING_SO.  Negative impact variables including:TEAM_BATTING_SO, TEAM_BASERUN_CS, TEAM_FIELDING_E,  TEAM_PITCHING_BB, TEAM_PITCHING_H, TEAM_PITCHING_HR. Outcome variable:TARGET_WINS.
 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(car)
library(caret)
library(corrplot)
library(data.table)
library(dplyr)
library(geoR)
library(ggplot2)
library(grid)
library(gridExtra)
library(knitr)
library(MASS)
library(naniar)
library(nortest)
library(psych)
library(testthat)
```


 1. DATA EXPLORATION (25 Points)  
 
Describe the size and the variables in the moneyball training data set.
a. Mean / Standard Deviation / Median 
b. Bar Chart or Box Plot of the data and/or Histograms 
c. Is the data correlated to the target variable (or to other variables?)  
d. Are any of the variables missing and need to be imputed "fixed"?

Glimpse:
Get a glimpse of our data. This is like a transposed version of print: columns run down the page, and data runs across. This makes it possible to see every column in a data frame. It's a little like str applied to a data frame but it tries to show you as much data as possible.We can see some missing data in TEAM_BASERUN_SB, TEAM_BASERUN_CS, TEAM_BATTING_HBP, TEAM_FIELDING_DP.  All the variable are interg.  

```{r, echo=FALSE, warning=FALSE, message=FALSE}
train <- read.csv("https://raw.githubusercontent.com/johnpannyc/data-621-HW1/master/moneyball-training-data.csv")
test <- read.csv("https://raw.githubusercontent.com/johnpannyc/data-621-HW1/master/moneyball-evaluation-data.csv")
glimpse(train)
```
Using summary, we can get the results of min, max, median, mean, 1st quarter, 3rd quater.  
```{r}
summary(train)
```
From the summary statistics, there are several variables that stand out due to extreme values. Such variables are TEAM_PITCHING_H and 30132.0. A deep dive into outlier analysis will be more informative regarding if these extreme values are correct or if they are generated by some data collection errors.

Let us use ggplot to overview the dependent variable:TARGET_WINS
```{r, echo=FALSE, fig.width=3, fig.height=3, cache=TRUE}
ggplot(train, aes(x=TARGET_WINS)) + geom_density()
```
The response variable is close to normal with a mean between 70 and 100. The plots are furthur complimented by providing descriptive statistics.



```{r}
boxplot(train,xlab="predictor comparitive")
```
 The boxplots below help show the spread of data within the dataset, and show various outliers. As shown in the graph below, TEAM_PITCHING_H seems to have the highest spread with the most outliers.



```{r}
train <- as.data.frame((train))

par(mfrow=c(3, 3))
colnames <- dimnames(train)[[2]]

  for(col in 2:ncol(train)) {

    d <- density(na.omit(train[,col]))
   #d <- qqnorm(na.omit(train[,col]))
    plot(d, type="n", main=colnames[col])
    polygon(d, col="blue", border="gray")
  }


```
In the Histograms, the data shows multiple graphs with right skews while only a few have left-skew.

Check missing data
```{r}
vis_miss(train)
```

```{r}
mb_cor <- cor(train)

round(mb_cor, 3)
```
check correlation
```{r}
M<-cor(train)
corrplot(M, method="number")
```









2. DATA PREPARATION (25 Points)  
Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations.  
a. Fix missing values (maybe with a Mean or Median value)  
b. Create flags to suggest if a variable was missing  
c. Transform data by putting it into buckets  
d. Mathematical transforms such as log or square root  
e. Combine variables (such as ratios or adding or multiplying) to create new variables

replace with outliers
```{r}
replaceOutliers = function(x) { 

    quantiles <- quantile( x, c(0.5,.95 ) )
    x[ x < quantiles[1] ] <- quantiles[1]
   
    x[ x > quantiles[2] ] <- quantiles[2]
    return(x)
}
   
train$TEAM_PITCHING_H <- replaceOutliers(train$TEAM_PITCHING_H)
```

Handling NA's
1. The variable TEAM_BATTING_HBP has 2085 NA's (almost 92% of the data). Including the variable in the analysis might not be the best approach. In addition, imputation of the variables with large percentage of NA's might not be an effective way to handle NA's. We will drop the variable from analysis.
The variable TEAM_BASERUN_CS has 34% missing values, we will remove the varible from the analusis. The remaining varibles with missing values will be treated by median imputation.

remove the index column
```{r}
train1 <- train[, -c(1)]
```


remove the TEAM_BATTING_HBP column
```{r}
train2 <- train1[, -c(10)]
```
remove the column of TEAM_BASERUN_CS
```{r}
train3 <- train2[, -c(9)]
```

Remove Collinearity
The variables TEAM_BATTING_HR and TEAM_PITCHING_HR are collinear with strong correlation of 97%. We will exclude the variable TEAM_PITCHING_HR to handle collinearity.

```{r}
train4 <- train3[, -c(9)]
```


replace with the missing values with their median value, respectively
```{r}
train4$TEAM_PITCHING_SO[is.na(train$TEAM_PITCHING_SO)] <- median(train4$TEAM_PITCHING_SO, na.rm = TRUE)
train4$TEAM_BATTING_SO[is.na(train$TEAM_BATTING_SO)] <- median(train4$TEAM_BATTING_SO, na.rm = TRUE)
train4$TEAM_BASERUN_SB[is.na(train$TEAM_BASERUN_SB)] <- median(train4$TEAM_BASERUN_SB, na.rm = TRUE)
train4$TEAM_FIELDING_DP[is.na(train$TEAM_FIELDING_DP)] <- median(train4$TEAM_FIELDING_DP, na.rm = TRUE)
```

```{r}
glimpse(train4)
```

Transforming the variables
  A closer look at the density plots form data exploration section reveals that the variables TEAM_BATTING_HR, TEAM_PITCHING_HR, TEAM_BATTING_SO and TEAM_PITCHING_SO can not be assumed normal.
Variable TEAM_PITCHING_HR was removed due to multicollinearity. We will transform the remaining three variables for normality assumption. The variables will be transformed using log transformations.

```{r}
train_t <- train4
train_t$TEAM_BATTING_HR_tr <- log(train_t$TEAM_BATTING_HR +1)
train_t$TEAM_BATTING_SO_tr <- log(train_t$TEAM_BATTING_SO +1)
train_t$TEAM_PITCHING_SO_tr <- log(train_t$TEAM_PITCHING_SO +1)
```



New Variable Creation
  Because TEAM_BATTING_H including TEAM_BATTING_1B, TEAM_BATTING_2B, TEAM_BATTING_3B, TEAM_BATTING_HR, I substrac a few new variables (TEAM_BATTING_1B:single base hitsI)from TEAM_BATTING_H.
```{r}
data_clean <- train_t
data_clean <- data_clean %>% mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR) 
glimpse(data_clean)
```

After all the steps, we have got clean data. 

```{r}
summary(data_clean)
```

```{r}
boxplot(data_clean,xlab="predictor comparitive")
```
```{r}
vis_miss(data_clean)
```



3. BUILD MODELS (25 Points)  
Using the training data set, build at least three different multiple linear regression models, using different variables (or the same variables with different transformations). 

Backward elimination
    The Backward Elimination operator is a nested operator i.e. it has a subprocess. The subprocess of the Backward Elimination operator must always return a performance vector. The Backward Elimination operator starts with the full set of attributes and, in each round, it removes each remaining attribute of the given ExampleSet. For each removed attribute, the performance is estimated using the inner operators, e.g. a cross-validation. Only the attribute giving the least decrease of performance is finally removed from the selection. Then a new round is started with the modified selection. 
    

MODEL1:- Create model With all columns as features
```{r}
full.model <- lm (TARGET_WINS ~   . , data=data_clean)

reduced.full.model<- step (full.model, direction = "backward")    
```    

```{r}    
summary(reduced.full.model)  
```
    
MODEL2:- Create model With only significant columns (***) as features from the model 1.    
```{r}
significant.model <- lm (TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_3B + TEAM_BATTING_HR + TEAM_BASERUN_SB + TEAM_PITCHING_BB + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP + TEAM_BATTING_SO_tr + TEAM_PITCHING_SO_tr, data=data_clean)

reduced.significant.model<- step (significant.model, direction = "backward")
```
```{r}
summary(reduced.significant.model)
```
MODEL3:- Create model With top 5 high correlation columns as features
```{r}
cors <- sapply(data_clean, cor, y=data_clean$TARGET_WINS)
mask <- (rank(-abs(cors)) <= 6 )
best5.pred <- data_clean[, mask]

best5.pred <- subset(best5.pred, select = c(-TARGET_WINS) )
summary(best5.pred)
```
Stepwise backward regression    
```{r}    
full.model.best5 <- lm (TARGET_WINS ~     TEAM_BATTING_H + TEAM_BATTING_2B + TEAM_BATTING_BB + TEAM_BATTING_HR_tr + TEAM_BATTING_1B , data=data_clean)

reduced.model.best5<- step (full.model.best5, direction = "backward")    
```        

```{r}
summary(reduced.model.best5)
```


  



4. SELECT MODELS (25 Points)  
Decide on the criteria for selecting the best multiple linear regression model. Will you select a model with slightly worse performance if it makes more sense or is more parsimonious? Discuss why you selected your model.  For the multiple linear regression model, will you use a metric such as Adjusted R2, RMSE, etc.? Be sure to explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a) mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. Make predictions using the evaluation data set. 


Decide on the criteria for selecting the best multiple linear regression model.
Model 1 yield a slightly arge r square value, showing a slight better possible fit.  Although model 3 has a relative larger F statistic enven with only 5 predictor, we believe cover more variable is important to predict winning situation for Moneyball game.  Based on the above stats, Model 1 with all the columns as feature is a best fit. 

In order to use model 1 on the evaluation data. Firstly, we overview the data. Secondly, we did data cleaning, transformation and other manipulation which is exactly same as we did for train data.


```{r}
glimpse(test)
```
 
Model Application
replace with outliers
```{r}
replaceOutliers = function(x) { 

    quantiles <- quantile( x, c(0.5,.95 ) )
    x[ x < quantiles[1] ] <- quantiles[1]
   
    x[ x > quantiles[2] ] <- quantiles[2]
    return(x)
}
   
test$TEAM_PITCHING_H <- replaceOutliers(test$TEAM_PITCHING_H)
```  

remove the index column
```{r}
test1 <- test[, -c(1)]
```

remove the TEAM_BATTING_HBP column
```{r}
test2 <- test1[, -c(10)]
```

remove the column of TEAM_BASERUN_CS
```{r}
test3 <- test2[, -c(9)]
```

Remove Collinearity
The variables TEAM_BATTING_HR and TEAM_PITCHING_HR are collinear with strong correlation of 97%. We will exclude the variable TEAM_PITCHING_HR to handle collinearity.

```{r}
test4 <- test3[, -c(9)]
```

replace with the missing values with their median value, respectively
```{r}
test4$TEAM_PITCHING_SO[is.na(test$TEAM_PITCHING_SO)] <- median(test4$TEAM_PITCHING_SO, na.rm = TRUE)
test4$TEAM_BATTING_SO[is.na(test$TEAM_BATTING_SO)] <- median(test4$TEAM_BATTING_SO, na.rm = TRUE)
test4$TEAM_BASERUN_SB[is.na(test$TEAM_BASERUN_SB)] <- median(test4$TEAM_BASERUN_SB, na.rm = TRUE)
test4$TEAM_FIELDING_DP[is.na(test$TEAM_FIELDING_DP)] <- median(test4$TEAM_FIELDING_DP, na.rm = TRUE)
```

Transforming the variables
  A closer look at the density plots form data exploration section reveals that the variables TEAM_BATTING_HR, TEAM_PITCHING_HR, TEAM_BATTING_SO and TEAM_PITCHING_SO can not be assumed normal.
Variable TEAM_PITCHING_HR was removed due to multicollinearity. We will transform the remaining three variables for normality assumption. The variables will be transformed using log transformations.

```{r}
test_t <- test4
test_t$TEAM_BATTING_HR_tr <- log(test_t$TEAM_BATTING_HR +1)
test_t$TEAM_BATTING_SO_tr <- log(test_t$TEAM_BATTING_SO +1)
test_t$TEAM_PITCHING_SO_tr <- log(test_t$TEAM_PITCHING_SO +1)
```
New Variable Creation
  Because TEAM_BATTING_H including TEAM_BATTING_1B, TEAM_BATTING_2B, TEAM_BATTING_3B, TEAM_BATTING_HR, I substrac a few new variables (TEAM_BATTING_1B:single base hitsI)from TEAM_BATTING_H.
```{r}
test_data_clean <- test_t
test_data_clean <- data_clean %>% mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR) 
glimpse(test_data_clean)
```

```{r}
vis_miss(test_data_clean)
```
using the Model1 we can predict the dependent variable for the test data as follows
```{r}
test5 <- transform(test_data_clean)
test5["BATTING_1B"] <- NA
test5$BATTING_1B = test5$TEAM_BATTING_H - test5$TEAM_BATTING_HR - test5$TEAM_BATTING_3B - test5$TEAM_BATTING_2B
predictedValues <- predict (reduced.full.model, newdata=test5 )
summary (predictedValues)
```

 
 
 
 
 
